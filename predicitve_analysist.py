# -*- coding: utf-8 -*-
"""Predicitve Analysist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WfLJqSdEZIJZF9WE6xgtstTmHRjpVoE6
"""

#install kaggle
!pip install kaggle

#upload file json

from google.colab import files
files.upload()

#membuat directory kaggle dan dataset
!mkdir ~/.kaggle
!mkdir datasets

#Buat direktori bernama kaggle dan salin file kaggle.json di sana.
!cp kaggle.json ~/.kaggle/

#Ubah hak akses file
!chmod 600 ~/.kaggle/kaggle.json

#list kaggle dataset
!kaggle datasets list

"""## Business Understanding
Petani alpukat akan menjual alpukatnya lebih banyak pada daerah yang memiliki nilai permintaan alpukat yang tinggi dan akan menjual alpukatnya lebih sedikit pada daerah yang memiliki permintaan alpukat yang lebih sedikit.
### Problem Statement
Berdasarkan situasi diatas
- Bagaimana cara preprocessing pada data harga alpukat yang akan digunakan untuk membuat model yang baik?
- Bagaimana cara memilih/membuat model yang terbaik untuk memprediksi harga penjualan alpukat ?
### Goals
- Melakukan preprocessing data untuk model machine learning
- Membandingkan model score terbaik untuk memprediksi harga penjualan alpukat 
### Solution Statement
- Random Forest(RF). Pemilihan metode Random Forest sebagai metode prediksi pada penelitian ini didasari oleh kelebihannya yaitu dapat mengatasi noise dan missing value serta dapat mengatasi data dalam jumlah yang besar. Dan kekurangan pada algoritma Random Forest yaitu interpretasi yang sulit dan membutuhkan tuning model yang tepat untuk data. Cara kerja Random Forest yakni dengan memanggil fungsi RandomForestRegressor() yang telah diimport dari library scikit-learn[3].
- K-Nearest Neighbor(KNN). Pemilihan metode K-Nearest Neighbor sebagai metode prediksi pada penelitian ini didasari oleh kelebihannya yang mudah dipahami dan diimplementasikan, tangguh terhadap data training sample yang noisy, dan memiliki konsistensi yang kuat. Kekurangannya yakni perlu menentukan parameter k (jumlah tetangga terdekat), sensitif terhadap data outlier. Cara kerja K-Nearest Neighbor yakni dengan memanggil fungsi KNeighborsRegressor() yang telah diimport dari library scikit-learn[4].
- XGBoost (XGB) adalah pendekatan yang ampuh untuk membangun model regresi yang diawasi. Validitas pernyataan ini dapat disimpulkan dengan mengetahui tentang fungsi objektif (XGBoost) dan basis pembelajaran. Fungsi tujuan berisi fungsi kerugian dan istilah regularisasi. Ini menceritakan tentang perbedaan antara nilai aktual dan nilai prediksi, yaitu seberapa jauh hasil model dari nilai sebenarnya[5].
- Gradient Boosting (GB) adalah salah satu algoritma pembelajaran mesin paling populer untuk kumpulan data tabular. Ini cukup kuat untuk menemukan hubungan non linear antara target model dan fitur Anda dan memiliki kegunaan yang hebat yang dapat menangani nilai yang hilang, outlier, dan nilai kategorikal kardinalitas tinggi pada fitur Anda tanpa perlakuan khusus[6].
- LightGBM adalah kerangka peningkatan gradien yang menggunakan algoritma pembelajaran berbasis pohon. Ini dirancang untuk didistribusikan dan efisien[7].
"""

#download dataset dari kaggle
!kaggle datasets download -d neuromusic/avocado-prices

#unzip dataset yang telah didownload
!unzip /content/avocado-prices -d datasets/

# Commented out IPython magic to ensure Python compatibility.
#import library
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

# load the dataset
op = pd.read_csv('/content/datasets/avocado.csv')
op

"""## Data Understanding
Data yang digunakan adalah data yang ada pada kaggle [Avocado Prices](https://www.kaggle.com/datasets/neuromusic/avocado-prices)

keterangan columns pada the dataset:
Date - Tanggal Observasi data

- AveragePrice - Harga rata-rata pada satu buah alpukat adalah mata uang dollar
- type - Konvensional atau organik
- year - Tahun
- Region - Daerah observasi
- Total Volume - Jumlah alpukat yang terjual
- 4046 - Total jumlah alpukat dengan PLU 4046 terjual
- 4225 - Total jumlah alpukat dengan PLU 4225 terjual
- 4770 - Total jumlah alpukat dengan PLU 4770 terjual
##### Note
PLU merupakan Price look-up nomor berisi 4-5 digit untuk mengidentifikasikan suatu produk, berguna untuk memudahkan proses check-out dan juga memudahkan inventory control.


"""

#melihat ukuran data pada dataset
op.shape

#melihat informasi dataset
op.info

#melihat nilai numeric dataset
op.describe()

# Datanya belum bertype Date yang kolom Date jadi convert / ubah dulu
op['Date'] = pd.to_datetime(op['Date'])

#melihat informasi dataset
op.info()

# Cek perubahan harga data dari tahun ke tahun
plt.title(f'Data dari tahun ke tahun')
plt.plot(op['AveragePrice'], op['Date'])
plt.show()

# Kita coba cek sebaran dengan cycle per tahun
year = op['Date'].dt.year
year

# melihat nilai unik pada tahun 
year.unique()

#visualisasi data tiap tahun
for y in year.unique():
  plt.title(f'Tahun {y}')
  plt.plot(op[op['Date'].dt.year == y].AveragePrice)
  plt.show()

"""Setelah dilakukan visualisasi """

#visualisasi boxplot 
columns = np.delete(op.columns, 0)
for col in columns:
  plt.title(f'Column {col}')
  sns.boxplot(x="year", y="AveragePrice", data=op)
  plt.show()

# kita drop kolom yang tidak terpakai seperti date dan unnamed
new_op = op.drop('Date', axis=1)
new_op1 = new_op.drop('Unnamed: 0', axis=1)
new_op1

#melihat korelasi matrix
plt.figure(figsize=(10,10))
sns.heatmap(new_op1.corr(), annot=True, cmap='coolwarm',linewidths=0.5)
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)
plt.show()

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(new_op1, diag_kind = 'kde')

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot berdasarkan ukuran kantong
sns.pairplot(new_op1[['Small Bags','Large Bags','XLarge Bags','Total Bags']], plot_kws={"s": 4}, diag_kws=dict(fill=False));

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot berdasarkan jenis PLU
sns.pairplot(new_op1[['4046','4225','4770',]], plot_kws={"s": 4}, diag_kws=dict(fill=False));

"""## Data Preparation
 Teknik data preparation yang digunakan adalah: 
 - MinMaxScaller() dimana ketika menggunakan teknik ini kita harus menghilangkan kolum yang bernilai data type object. merupakan proses scalling yang fungsinya data numeric akan tahan terhadap pencilan data / outliers. MinMaxScaller ini mentransformasi / mengubah data numeric menjadi data numeric yang memiliki rentang 0 - 1
 - TrainTestSplit() untuk membagi dataset menjadi data latih (train) dan data uji (test) merupakan hal yang harus dilakukan sebelum membuat model. Mempertahankan sebagian data yang ada untuk menguji seberapa baik generalisasi model terhadap data baru. nilai yang digunakan untuk test adalah 0.2 atau 20% sehingga nilai yang digunakan untuk train adalah 0.8 atau 80% sehingga perbandingan rasio train/test adalah 80:20.



"""

#menunjukan isi dari dataset
new_op1.head()

#untuk melakukan MinMaxScaller kita drop region dan type pada tabel
from sklearn.preprocessing import MinMaxScaler
new_op2 = new_op1.drop('type', axis=1)
new_op3 = new_op2.drop('region', axis=1)

scaler = MinMaxScaler()

final_op = new_op3
scaler.fit(final_op[final_op.columns])
final_op[final_op.columns] = scaler.transform(final_op.loc[:, final_op.columns])

final_op

#melihat deskripsi data
final_op.describe()

# Abis di Standar tinggal di split, jadi data test tinggal tembak aja pake pred
from sklearn.model_selection import train_test_split

X, y = final_op.drop('AveragePrice', axis=1), final_op['AveragePrice']

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=767)

"""#Modeling

## Modeling

Pada tahap ini mengembangkan model machine learning dengan lima algoritma, yakni XGBoost, K-Nearest Neighbor, Random Forest, Gradient Boosting, LightGBM. Langkah selanjutnya yakni mengevaluasi performa masing-masing algoritma dan menentukan algoritma mana yang memberikan hasil prediksi terbaik. Langkah pertama dalam proses modeling yakni menyiapkan sebuah DataFrame baru untuk menampung berapa nilai mae-nya yang berfungsi pada proses analisis model.

 #### Random Forest (RF)
Random Forest = Random Forest dapat digunakan sebagai regresi dengan memperluas 'tree' sepenuhnya sehingga setiap daun memiliki tepat satu nilai. Breiman menyarankan untuk membuat regresi random forest dengan cara memperluas pohon secara acak. Kemudian sebuah prediksi secara sederhana mengembalikan variabel respon individual dari distribusi dapat dibangun jika 'forest' cukup besar. Satu peringatan bahwa perkembangan 'tree' sepenuhnya dapat menutupi atau melebihi kapasitas: jika itu terjadi, intervalnya akan sia-sia, seperti prediksi. Hal yang diharapkan adalah sama seperti akurasi dan presisi.
- n_estimators — jumlah pohon keputusan yang akan Anda jalankan dalam model
- max_depth — ini mengatur kedalaman maksimum yang mungkin dari setiap pohon
- n_jobs — jumlah tugas yang akan dijalankan secara paralel. -1 berarti menggunakan semua prosesor
- random_state  — yang merupakan random number generator
 ##### Keuntungan 
 - RF dapat menyelesaikan kedua jenis masalah yaitu klasifikasi dan regresi dan melakukan estimasi yang layak di kedua sisi.
 - Memiliki metode yang efektif untuk memperkirakan data yang hilang dan menjaga akurasi ketika sebagian besar data hilang.
 ##### Kekurangan 
 - seperti pendekatan kotak hitam untuk pemodel statistik, kami hanya memiliki sedikit kendali atas apa yang dilakukan model tersebut.
 
 #### XGBoost
XGBoostadalah implementasi sumber terbuka yang populer dan efisien dari algoritma pohon yang ditingkatkan gradien. Peningkatan gradien adalah algoritma pembelajaran yang diawasi, yang mencoba memprediksi variabel target secara akurat dengan menggabungkan perkiraan serangkaian model yang lebih sederhana dan lebih lemah. Saat menggunakangradien meningkatkanuntuk regresi, peserta didik yang lemah adalah pohon regresi, dan setiap pohon regresi memetakan titik data input ke salah satu daunnya yang berisi skor berkelanjutan. XGBoost meminimalkan fungsi obyektif yang diatur (L1 dan L2) yang menggabungkan fungsi kehilangan cembung (berdasarkan perbedaan antara output yang diprediksi dan target) dan istilah penalti untuk kompleksitas model (dengan kata lain, fungsi pohon regresi).
- n_estimators = 100 (Default)
- subsample = None (Default)
- max_depth = None (Default)
 ##### Keuntungan
 - Efektif dengan kumpulan data besar. Algoritme pohon seperti XGBoost dan Random Forest tidak memerlukan fitur yang dinormalisasi dan bekerja dengan baik jika datanya nonlinier, nonmonotonik, atau dengan kluster terpisah.
 ##### Kekurangan
 - Tidak bekerja dengan baik apabila data tidak terstruktur
 
 #### LGBM
LightGBM mengimplementasikan algoritma Gradient Boosting Decision Tree (GBDT) konvensional dengan penambahan dua teknik baru: Pengambilan Sampel Satu Sisi Berbasis Gradien (GOSS) dan Bundling Fitur Eksklusif (EFB). Teknik-teknik ini dirancang untuk secara signifikan meningkatkan efisiensi dan skalabilitas GBDT. Algoritma LightGBM berkinerja baik dalam kompetisi Machine Learning karena penanganannya yang kuat dari berbagai jenis data, hubungan, distribusi, dan keragaman hyperparameter yang dapat  disempurnakan.
- n_estimators = 100 (Default)
- subsample = None (Default)
- max_depth = None (Default)
 ##### Keuntungan
 - Kecepatan latihan lebih cepat dan efisiensi lebih tinggi
 - Menggunakan memori yang relatif kecil
 ##### Kekurangan
 - Light GBM peka terhadap overfitting dan karenanya dapat dengan mudah membuat data kecil overfitting
 
#### KNN
KNN bekerja berdasarkan prinsip bahwa setiap titik data yang berdekatan satu sama lain akan berada di kelas yang sama. Dengan kata lain, KNN mengklasifikasikan titik data baru berdasarkan kemiripan.
 - n_neighbors — jumlah tetangga untuk digunakan secara default untuk kueri k tetangga.
##### Keuntungan
- Pemodelan KNN tidak termasuk periode pelatihan karena data itu sendiri adalah model yang akan menjadi acuan untuk prediksi masa depan dan karenanya sangat efisien waktu dalam hal improvisasi untuk pemodelan acak pada data yang tersedia.
##### kekurangan 
- Tidak bekerja dengan baik dengan kumpulan data besar.

#### Gradient Boosting
cara kerja algoritma gradient boost adalah membangun satu tree untuk menyesuaikan data, lalu tree berikutnya dibangun untuk mengurangi residual (error).
- n_estimators = 100 (Default)
- subsample = None (Default)
- max_depth = None (Default)
##### Keuntungan
- dapat mengoptimalkan berbagai fungsi kerugian dan menyediakan beberapa opsi penyetelan parameter hiper yang membuat fungsi tersebut sangat fleksibel
##### Kekurangan
- Model Peningkatan Gradien akan terus ditingkatkan untuk meminimalkan semua kesalahan. Ini dapat terlalu menekankan outlier dan menyebabkan overfitting.



"""

#import library
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Siapkan dataframe untuk analisis model
report = pd.DataFrame(index=['train_mse', 'test_mse'], 
                      columns=['GradientBoosting', 'RandomForest', 'XGB', 'LGBM'])

# buat model prediksi
# model Random Forest
# n_estimator = number of tree in the forest
# max_depth = max kedalaman tree
# n_jobs = -1 using all processor
RF = RandomForestRegressor(n_estimators=100, random_state=55, n_jobs=-1)
RF.fit(x_train, y_train)

#Model XGradient boosting
XGB = GradientBoostingRegressor()
XGB.fit(x_train, y_train)

#Model Light GBM
LGBM = LGBMRegressor()
LGBM.fit(x_train, y_train)

#Model GB
GB = GradientBoostingRegressor()
GB.fit(x_train, y_train)

#MOdel KNN
from sklearn.neighbors import KNeighborsRegressor
knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(x_train, y_train)

"""#Evaluation

- Semakin kecil nilai RMSE, semakin dekat nilai yang diprediksi dan diamati dengan kata lain prediksi semakin akurat.
- Nilai Mean Squared Error yang rendah atau nilai mean squared error mendekati nol menunjukkan bahwa hasil peramalan sesuai dengan data aktual dan bisa dijadikan untuk perhitungan peramalan di periode mendatang.
- Semakin nilai r2 kecil maka artinya komponen error yang besar

Metrik evaluasi yang digunakan adalah:
### Mean Squared Error (MSE)
Mean Squared Error (MSE) adalah Rata-rata Kesalahan kuadrat diantara nilai aktual dan nilai prediksi. Metode Mean Squared Error secara umum digunakan untuk mengecek estimasi berapa nilai kesalahan pada prediksi. Nilai Mean Squared Error yang rendah atau nilai mean squared error mendekati nol menunjukkan bahwa hasil prediksi sesuai dengan data aktual dan bisa dijadikan untuk perhitungan prediksi di periode mendatang. Metode Mean Squared Error biasanya digunakan untuk mengevaluasi metode pengukuran dengan model regressi.
### Root  Mean Squared Error (RMSE)
Root Mean Squared Error (RMSE) merupakan salah satu cara untuk mengevaluasi model regresi dengan mengukur tingkat akurasi hasil perkiraan suatu model. RMSE dihitung dengan mengkuadratkan error (prediksi – observasi) dibagi dengan jumlah data (= rata-rata), lalu diakarkan.
Nilai RMSE rendah menunjukkan bahwa variasi nilai yang dihasilkan oleh suatu model prakiraan mendekati variasi nilai obeservasinya. RMSE menghitung seberapa berbedanya seperangkat nilai. Semakin kecil nilai RMSE, semakin dekat nilai yang diprediksi dan diamati.
### R2 Score
R squared merupakan angka yang berkisar antara 0 sampai 1 yang mengindikasikan besarnya kombinasi variabel independen secara bersama – sama mempengaruhi nilai variabel dependen. Semakin mendekati angka satu, model yang dikeluarkan oleh regresi tersebut akan semakin baik.
Jika kita perhatikan rumus R squared dibawah sangat dipengaruhi oleh nilai Y prediksi atau nilai Y dari hasil rumus dengan nilai Y aktual. Kenyataan yang sering muncul adalah nilai R squared akan semakin membaik (nilainya akan terus mendekati nilai 1) jika kita menambah variabel. Semakin banyak jumlah variabel yang menentukan nilai Y prediksi, maka nilai SSR akan semakin besar yang berakibat pada besarnya nilai R squared.



"""

#menentukan nilai MSE dan R2 pada setiap model
hasil_akhir = {'Model_Name': [], 'mse': [], 'r2': []}
pred = GB.predict(x_test)
mse = mean_squared_error(y_true=y_test, y_pred=pred)
r2 = r2_score(y_test, pred)
hasil_akhir['Model_Name'].append('GB')
hasil_akhir['mse'].append(mse)
hasil_akhir['r2'].append(r2)
pred = LGBM.predict(x_test)
mse = mean_squared_error(y_true=y_test, y_pred=pred)
r2 = r2_score(y_test, pred)
hasil_akhir['Model_Name'].append('LGBM')
hasil_akhir['mse'].append(mse)
hasil_akhir['r2'].append(r2)
pred = RF.predict(x_test)
mse = mean_squared_error(y_true=y_test, y_pred=pred)
r2 = r2_score(y_test, pred)
hasil_akhir['Model_Name'].append('RF')
hasil_akhir['mse'].append(mse)
hasil_akhir['r2'].append(r2)
pred = XGB.predict(x_test)
mse = mean_squared_error(y_true=y_test, y_pred=pred)
r2 = r2_score(y_test, pred)
hasil_akhir['Model_Name'].append('XGB')
hasil_akhir['mse'].append(mse)
hasil_akhir['r2'].append(r2)
pred = knn.predict(x_test)
mse = mean_squared_error(y_true=y_test, y_pred=pred)
r2 = r2_score(y_test, pred)
hasil_akhir['Model_Name'].append('knn')
hasil_akhir['mse'].append(mse)
hasil_akhir['r2'].append(r2)

#melihat hasil dari nilai MSE,R2 pada setiap model
hasil_akhir

#karena angkanya terlalu banyak jadi kita ambil 5 angka dibelakang koma
pd.options.display.float_format = '{:.5f}'.format

#menunjukan hasil akhir dengan tampilan dataframe
hasil_akhir = pd.DataFrame.from_dict(hasil_akhir)
hasil_akhir

#menentukan nilai RMSE
hasil_akhir['rmse'] = np.sqrt(hasil_akhir['mse'])

#menampilkan hasil akhir setelah ditambahkan RMSE
hasil_akhir

#nilai XGB,LGBM GB sama RF sama mse, rmse, r2 nya
model_dict = {'RF': RF, 'XGB': XGB, 'GB': GB, 'LGBM': LGBM, 'knn': knn}
prediksi = x_test.iloc[:180].copy()
pred_dict = {'y_true':y_test[:180]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)
 
pd.DataFrame(pred_dict)

#menampilkan score dari setiap model
print("Accuracy score dari model LGBM              = ", LGBM.score(x_test, y_test))
print("Accuracy score dari model GB                = ", GB.score(x_test, y_test))
print("Accuracy score dari model XGB               = ", XGB.score(x_test, y_test))
print("Accuracy score dari model Random Forest     = ", RF.score(x_test, y_test))
print("Accuracy score dari KNN                     = ", knn.score(x_test, y_test))

"""## Kesimpulan
Setelah melalui berbagai tahapan evaluasi dan membandingkan ke-5 algoritma yang digunakan yakni, LGBM, XGBoost, KNN, Gradient Boosting dan Random Forest. Nilai skor terbesar adalah algoritma Random Forest dengan nilai 81% disusul oleh LGBM dengan score 76% kemudian KNN dengan skor 73% dan Gradient Boosting dan XGboost dengan skor 64%
"""